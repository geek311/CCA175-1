1)	Import orders table from mysql as text file to the destination /user/nikunjpatel1989/problem5/text. 
	Fields should be terminated by a tab character ("\t") character and lines should be terminated by new line character ("\n"). 

Solution:
	
sqoop import \
--connect "jdbc:mysql://101.53.130.146/nikunjpatel1989" \
--username nikunjpatel1989 \
--password HZj4zKduxtwoKNm \
--table ordermaster \
--target-dir /user/nikunjpatel1989/problem5/text \
--as-textfile \
--fields-terminated-by '\t' \
--lines-terminated-by '\n' \
-m 1;
	
2)	Import orders table from mysql  into hdfs to the destination /user/nikunjpatel1989/problem5/avro. File should be stored as avro file.

Solution:

sqoop import \
--connect "jdbc:mysql://101.53.130.146/nikunjpatel1989" \
--username nikunjpatel1989 \
--password HZj4zKduxtwoKNm \
--table ordermaster \
--target-dir /user/nikunjpatel1989/problem5/avro \
--as-avrodatafile \
-m 1;

3)	Import orders table from mysql  into hdfs  to folders /user/nikunjpatel1989/problem5/parquet. File should be stored as parquet file.

Solution:

sqoop import \
--connect "jdbc:mysql://101.53.130.146/nikunjpatel1989" \
--username nikunjpatel1989 \
--password HZj4zKduxtwoKNm \
--table ordermaster \
--target-dir /user/nikunjpatel1989/problem5/parquet \
--as-parquetfile \
-m 1;

4)	Transform/Convert data-files at /user/nikunjpatel1989/problem5/avro and store the converted file at the following locations and file formats
	save the data to hdfs using snappy compression as parquet file at /user/nikunjpatel1989/problem5/parquet-snappy-compress
	save the data to hdfs using gzip compression as text file at /user/nikunjpatel1989/problem5/text-gzip-compress
	save the data to hdfs using no compression as sequence file at /user/nikunjpatel1989/problem5/sequence
	save the data to hdfs using snappy compression as text file at /user/nikunjpatel1989/problem5/text-snappy-compress

Solution:

import com.databricks.spark.avro._

var dataFile = sqlContext.read.avro("/user/nikunjpatel1989/problem5/avro")

dataFile.repartition(1).write.parquet("/user/nikunjpatel1989/problem5/parquet-snappy-compress")
dataFile.map(x=> x(0)+","+x(1)+","+x(2)+","+x(3)).take(5).foreach(println)
dataFile.map(x=> x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/nikunjpatel1989/problem5/text-gzip-compress",classOf[org.apache.hadoop.io.compress.GzipCodec]);
dataFile.map(x=>(x(0).toString,x(0)+","+x(1)+","+x(2)+","+x(3))).saveAsSequenceFile("/user/nikunjpatel1989/problem5/sequence")
dataFile.map(x=> x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/nikunjpatel1989/problem5/text-snappy-compress",classOf[org.apache.hadoop.io.compress.SnappyCodec])
	
5)	Transform/Convert data-files at /user/nikunjpatel1989/problem5/parquet-snappy-compress and store the converted file at the following locations and file formats
	save the data to hdfs using no compression as parquet file at /user/nikunjpatel1989/problem5/parquet-no-compress
	save the data to hdfs using snappy compression as avro file at /user/nikunjpatel1989/problem5/avro-snappy

Solution:

var parquetDataFile = sqlContext.read.parquet("/user/nikunjpatel1989/problem5/parquet-snappy-compress")
sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
parquetDataFile.write.parquet("/user/nikunjpatel1989/problem5/parquet-no-compress")

sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
parquetDataFile.write.avro("/user/nikunjpatel1989/problem5/avro-snappy")
	
6)	Transform/Convert data-files at /user/nikunjpatel1989/problem5/avro-snappy and store the converted file at the following locations and file formats
	save the data to hdfs using no compression as json file at /user/nikunjpatel1989/problem5/json-no-compress
	save the data to hdfs using gzip compression as json file at /user/nikunjpatel1989/problem5/json-gzip

Solution:

import com.databricks.spark.avro._
var avroData = sqlContext.read.avro("/user/nikunjpatel1989/problem5/avro-snappy")
avroData.toJSON.saveAsTextFile("/user/nikunjpatel1989/problem5/json-no-compress")
avroData.toJSON.saveAsTextFile("/user/nikunjpatel1989/problem5/json-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])
	
7)	Transform/Convert data-files at  /user/nikunjpatel1989/problem5/json-gzip and store the converted file at the following locations and file formats
	save the data to as comma separated text using gzip compression at   /user/nikunjpatel1989/problem5/csv-gzip

Solution:

var jsonData = sqlContext.read.json("/user/nikunjpatel1989/problem5/json-gzip")
jsonData.map(x=>x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/nikunjpatel1989/problem5/csv-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])
	
8)	Using spark access data at /user/nikunjpatel1989/problem5/sequence and stored it back to hdfs using no compression as ORC file to HDFS to destination /user/nikunjpatel1989/problem5/orc

Solution:

Firstly read the sequence file and get its key and value from the first line.
hadoop fs -get /user/nikunjpatel1989/problem5/sequence/part-00000
cut  -c-300 part-00000

sc.sequenceFile("/user/nikunjpatel1989/problem5/sequence",classOf[org.apache.hadoop.io.Text],classOf[org.apache.hadoop.io.Text])
seqData.map(x=>x._2.toString).take(5).foreach(println)
seqData.map(x=>{ var d = x._2.toString.split(",");(d(0),d(1),d(2),d(3))}).toDF().write.orc("/user/nikunjpatel1989/problem5/orc")